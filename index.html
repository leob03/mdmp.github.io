<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="MDMP: Multi-modal Diffusion for Supervised Motion Predictions with Uncertainty.">
  <meta name="keywords" content="MDMP, Multi-modal Diffusion, Motion Prediction, Uncertainty, Human-Robot Collaboration">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MDMP: Multi-modal Diffusion for Supervised Motion Predictions with Uncertainty</title>

  <!-- Fonts and CSS -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <!--
      <a class="navbar-item" href="https://your-personal-homepage-or-lab-page.com">
        <span class="icon"><i class="fas fa-home"></i></span>
      </a>
      -->

      <!--
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">More Projects</a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/YOUR-GITHUB-USERNAME/OTHER-PROJECT">Other Project 1</a>
          <a class="navbar-item" href="https://github.com/YOUR-GITHUB-USERNAME/ANOTHER-PROJECT">Other Project 2</a>
        </div>
      </div>
      -->
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MDMP: Multi-modal Diffusion for Supervised Motion Predictions with Uncertainty</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://leobringer.com/about-me/">Leo Bringer,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=_AtReX8AAAAJ&hl=en">Joey Wilson,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=RXmPJqsAAAAJ&hl=en">Kira Barton,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=l2jdSb8AAAAJ&hl=en">Maani Ghaffari,
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Michigan,</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2410.03860.pdf" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper (PDF)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.03860" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
                </a>
              </span>
              <!-- If you have a demo video: -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=YOUR_VIDEO_ID" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-youtube"></i></span><span>Video</span>
                </a>
              </span>
              <!-- Code link: -->
              <span class="link-block">
                <a href="https://github.com/leob03/mdmp" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span><span>Code</span>
                </a>
              </span>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- Optional: Teaser video or image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/videos/short.gif" alt="Teaser GIF" height="100%">
      <h2 class="subtitle has-text-centered">
        MDMP enables long-term human motion predictions with quantifiable uncertainty.
      </h2>
    </div>
  </div>
</section>

<!-- Move Audio Overview here -->
<section class="section has-background-white">
  <div class="container is-max-desktop">
    <!-- Audio Overview -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Audio Overview (generated using <a href="https://notebooklm.google.com/?original_referer=https:%2F%2Fresearch.nvidia.com%23&pli=1&authuser=3">NotebookLM</a>)</h2>
        <div class="content has-text-centered">
          <audio controls>
            <source src="./static/audio/podcast.wav" type="audio/wav">
            Your browser does not support the audio element.
          </audio>
          <p class="is-size-6" style="font-style: italic; margin-top: 10px;">
            Listen to a high-level overview of MDMP.
          </p>
        </div>
      </div>
    </div>
    <!-- /Audio Overview -->
  </div>
</section>

<section class="section has-background-white">
  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper introduces a Multi-modal Diffusion model for Motion Prediction (MDMP) that integrates and synchronizes skeletal data and textual descriptions of actions to generate refined long-term motion predictions with quantifiable uncertainty.
          </p>
          <p>
            Existing methods for motion forecasting or generation rely solely on motion or text inputs, limiting precision or control over extended durations. Our multi-modal approach enhances contextual understanding, while a graph-based transformer effectively captures spatio-temporal dynamics. Consequently, MDMP consistently outperforms existing methods in accurately predicting long-term motions. By leveraging diffusion models’ ability to capture different modes of prediction, we estimate uncertainty and significantly improve spatial awareness in human-robot interactions.
          </p>
        </div>
      </div>
    </div>
    <!-- /Abstract -->

    <!-- Method -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <!-- Add the image summarizing the method -->
          <img src="./static/images/method.png" alt="Method Summary" style="margin-bottom: 20px;">
          <p>
            As part of the Diffusion Process MDMP progressively denoises a motion sample conditioned by an input motion through masking. Our architecture employs a GCN encoder to capture spatial joint features. We encode text prompts using CLIP followed by a linear layer; the textual embedding c and the noise time-step t are projected to the same dimensional latent space by separate feed-forward networks. These features, summed with a sinusoidal positional embedding, are fed into a Transformer encoder-only backbone. The backbone output is projected back to the original motion dimensions via a GCN decoder. Our model is trained both conditionally and unconditionally on text, by randomly masking 10% of the text embeddings. This approach balances diversity and text-fidelity during sampling.
          </p>
          <p>
            Our method uses the building blocks of <a href="https://guytevet.github.io/mdm-page/">MDM</a>, but with three key differences: (1) a denoising model that includes variance learning to increase log-likelihood and perform uncertainty estimates, (2) the GCN encoder with learnable graph connectivity, and (3) a learning framework that incorporates contextuality by synchronizing skeletal inputs with initial textual inputs.
          </p>
        </div>
      </div>
    </div>
    <!-- /Method -->

    <!-- Main Results -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Main Results</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/YOUR_VIDEO_ID?rel=0&showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!-- Main Results -->

  </div>
</section>


<section class="section has-background-light">
  <div class="container is-max-desktop">
    <!-- Results -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <h3>Model Accuracy Evaluation:</h3>
          <img src="./static/images/results_.png" alt="Model Accuracy Evaluation" style="margin-bottom: 20px;">
          <ul>
            <li>MDMP outperforms baseline Text2Motion models in terms of accuracy, particularly over longer sequences. Unlike baseline models like MoMask, MotionGPT, and MDM that treat motion data as a masked input during sampling, MDMP is trained to leverage it as an additional supervision signal. This leads to significant performance improvement, demonstrated by lower MPJPE values over time.</li>
            <li>Integrating textual and skeletal data significantly enhances the accuracy of predictions. Ablation studies (Table 1) confirm that combining both input types results in much higher prediction accuracy.</li>
          </ul>
          <h3>Uncertainty Parameter Evaluation:</h3>
          <img src="./static/images/sparsification.png" alt="Uncertainty Parameter Evaluation" style="margin-bottom: 20px;">
          <ul>
            <li>Mode Divergence proves to be the best performing uncertainty index. It closely follows the Oracle curve, indicating strong alignment between uncertainty estimates and true errors.</li>
            <li>Denoising Fluctuations and Predicted Variance are less reliable indices for uncertainty estimation. While they show a general declining trend, the effect is less pronounced suggesting less reliability.</li>
          </ul>
          <h3>Ablation Study - Motion and Text Effects:</h3>
          <img src="./static/images/table1.png" alt="Ablation Study - Motion and Text Effects" style="margin-bottom: 20px;">
          <ul>
            <li>The study confirms the importance of multimodal fusion by demonstrating increased prediction accuracy when both input types are used.</li>
            <li>The model relies more heavily on motion input sequences than textual prompts for short-term predictions.</li>
            <li>Textual information is most useful for longer-term predictions where the stochasticity and variability of potential scenarios are much higher.</li>
          </ul>
          <h3>Ablation Study - Architectural Design and Parameter Choice:</h3>
          <img src="./static/images/table2.png" alt="Ablation Study - Architectural Design and Parameter Choice" style="margin-bottom: 20px;">
          <ul>
            <li>Learned Graph Connectivity improves the understanding of human joint trajectory dependencies. Using GCNs leads to better performance than linear layers, especially for longer-term predictions.</li>
            <li>Learning variances allows the model to capture more data distribution modes, enhancing the accuracy over longer-term predictions.</li>
            <li>Reducing the number of diffusion steps significantly improves the computational efficiency, which is pivotal for real-time Human-Robot Collaboration. This optimization also resulted in improved accuracy over time.</li>
          </ul>
        </div>
      </div>
    </div>
    <!-- /Results -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{bringer2024mdmp,
      title={MDMP: Multi-modal Diffusion for supervised Motion Predictions with uncertainty}, 
      author={Leo Bringer and Joey Wilson and Kira Barton and Maani Ghaffari},
      year={2024},
      eprint={2410.03860},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.03860},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- Remove or change if you don’t want these icons -->
      <a class="icon-link" href="YOUR_PDF_LINK">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/YOUR-GITHUB-USERNAME" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from a template under a
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Source code of this website template is available
            <a href="https://github.com/nerfies/nerfies.github.io">here</a>. Remember to remove or update analytics code if present.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
